\documentclass[12pt]{article}

\title{Problem Set 8: The Beta -- Binomial Model}
\author{MATH E-158: Introduction to Bayesian Inference}
\author{David Shaub}
\date{Due November 20, 2017}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\renewcommand\qedsymbol{$\blacksquare$}
	
\begin{document}
	
	\maketitle


\section*{Problem 1}


\subsection*{Problem Statement}

\bigskip
For each of these functions, determine if it is the kernel of a beta distribution, and if so, what the values of the parameters $a$ and $b$ are:
\begin{itemize}
	\item {\bf Part (a)} $g(p\ |\ a,b) = p \cdot (1 - p)$.
	\item {\bf Part (b)} $g(p\ |\ a,b) = p$
	\item {\bf Part (c)} $g(p\ |\ a,b) = \displaystyle \frac{ p^3}{\sqrt{1 - p} }$
	\item {\bf Part (d)} $g(p\ |\ a,b) = p^4 + (1 - p)^3$
	\item {\bf Part (e)} $g(p\ |\ a,b) = \sqrt[3]{p} - 2 \sqrt[3]{p} \cdot p + \sqrt[3]{p} \cdot p^2 $
\end{itemize}



\subsection*{Problem Solution}
\begin{itemize}
	\item {\bf Part (a)} $g(p\ |\ a,b) = p \cdot (1 - p)$.\\
	Yes, $a=2$ and $b=2$
	\item {\bf Part (b)} $g(p\ |\ a,b) = p$\\
	Yes, $a=2$ and $b=1$
	\item {\bf Part (c)} $g(p\ |\ a,b) = \displaystyle \frac{ p^3}{\sqrt{1 - p} }$\\
	Yes, $a=4$ and $b=0.5$
	\item {\bf Part (d)} $g(p\ |\ a,b) = p^4 + (1 - p)^3$\\
	No
	\item {\bf Part (e)} $g(p\ |\ a,b) = \sqrt[3]{p} - 2 \sqrt[3]{p} \cdot p + \sqrt[3]{p} \cdot p^2 $\\
	Yes, $a=4/3$ and $b=3$
\end{itemize}


\newpage
\section*{Problem 2}

\subsection*{Problem Statement}

Tyrone has wants to analyze free throws in basketball. He decides to model an individual's probability of success with a beta distribution, and he wants this beta distribution to have mean $\mu = 0.6$ and standard deviation $\sigma = 0.10$. 

\bigskip
\noindent
{\bf Part (a)} What are the hyperparameters $a$ and $b$ for Tyrone's prior beta distribution?

\bigskip
\noindent
{\bf Part (b)} Ashley attempts 20 free throws, and she makes 18 of them. What are the hyperparameters for Tyrone's posterior distribution for Ashley?

\bigskip
\noindent
{\bf Part (c)} What is Tyrone's estimate of Ashley's true free-throw probability of success, given the observed data?



\subsection*{Problem Solution}

\noindent
{\bf Part (a)} What are the hyperparameters $a$ and $b$ for Tyrone's prior beta distribution?

\noindent
{\bf Solution} 
\begin{align*}
\sigma^2 &= 0.01\\
a &= \frac{0.6^2 - 0.6^3 - 0.01 \cdot 0.6}{0.01}\\
&= 13.8\\
b &= \frac{13.8 \cdot (1 - 0.6)}{0.6}\\
&= 9.2
\end{align*}



\newpage
\subsubsection*{Problem 2, continued}
\noindent
{\bf Part (b)} Ashley attempts 20 free throws, and she makes 18 of them. What are the hyperparameters for Tyrone's posterior distribution for Ashley?

\noindent
{\bf Solution} 
\begin{align*}
a' &= a + k = 13.8 + 18 = 31.8\\
b' &= b + N - k = 9.2 + 20 - 18 = 11.2
\end{align*}


\noindent
{\bf Part (c)} What is Tyrone's estimate of Ashley's true free-throw probability of success, given the observed data?

\bigskip
\noindent
{\bf Solution} 
\begin{align*}
E[p|data] &= \frac{a'}{a' + b'}\\
&= \frac{31.8}{31.8 + 11.2}\\
&= 0.7395349\\
\end{align*}







\newpage
\section*{Problem 3}

\subsection*{Problem Statement}

Obie has decided to take up baseball, and Elvis and Taylor would like to estimate his true batting average. They both agree to use for their prior a beta distribution with hyperparameters $a = 13$ and $b = 37$. Taylor watches Obie during practice, and in the morning he gets 11 hits out of 25 at-bats, while in the afternoon he gets 7 hits out of 15 at-bats.

\bigskip
\noindent
{\bf Part (a)} Taylor updates the intitial prior distribution after the morning practice, using the data observed up to that time. What are the hyperparameters for her posterior distribution after the morning practice?

\bigskip
\noindent
{\bf Part (b)} At the end of the day, Taylor uses the data from afternoon practice to update the posterior from the morning. What are the hyperparameters for her posterior distribution after the afternoon practice?


\bigskip
\noindent
{\bf Part (c)} Elvis doesn't attend batting practice, and that evening he learns the total number of Obie's hits and at-bats for the entire day, without the morning and afternoon subtotals. Elvis then uses the data for the entire day to update his initial prior distribution in one step. What are the hyperparameters for his posterior distribution at the end of the day?


\subsection*{Problem Solution}

\noindent
{\bf Part (a)} Taylor updates the intitial prior distribution after the morning practice, using the data observed up to that time. What are the hyperparameters for her posterior distribution after the morning practice?


\bigskip
\noindent
{\bf Solution} 
\begin{align*}
a' &= a + k = 13 + 11 = 24\\
b' &= b + N - k = 37 + 25 - 11 = 51
\end{align*}

\newpage
\subsubsection*{Problem 3, continued}

\noindent
{\bf Part (b)} At the end of the day, Taylor uses the data from afternoon practice to update the posterior from the morning. What are the hyperparameters for her posterior distribution after the afternoon practice?

\noindent
{\bf Solution} 
\begin{align*}
a' &= a + k = 24 + 7 = 31\\
b' &= b + N - k = 51 + 15 - 7 = 59
\end{align*}


\noindent
{\bf Part (c)} Elvis doesn't attend batting practice, and that evening he learns the total number of Obie's hits and at-bats for the entire day, without the morning and afternoon subtotals. Elvis then uses the data for the entire day to update his initial prior distribution in one step. What are the hyperparameters for his posterior distribution at the end of the day?

\bigskip
\noindent
{\bf Solution} 

\begin{align*}
k &= 11 + 7 = 18\\
N &= 15 + 25 = 40\\
a' &= a + k = 13 + 18 = 31\\
b' &= b + N - k = 37 + 40 - 18 = 59
\end{align*}





\newpage
\section*{Problem 4}

\subsection*{Problem Statement}

Obie has now made it to the big leagues! Joe and Grant decide to analyze Obie's batting average, with both using a beta distribution as the prior for $p$, Obie's true batting average. Joe has a prior with hyperparameters $a = 20$ and $b = 60$, while Grant has hyperparameters $a = 75$ and $b = 225$.

\bigskip
\noindent
{\bf Part (a)} What are the expected values for both Joe and Grant's prior distributions?

\bigskip
\noindent
{\bf Part (b)} What are the standard deviations for both Joe and Grant's prior distributions?

\bigskip
\noindent
{\bf Part (c)} Obie gets 80 hits in his first 200 at-bats. What is Joe's posterior estimate of Obie's true batting average?

\bigskip
\noindent
{\bf Part (d)} Grant observes the same 80 hits in 200 at-bats that Joe did. What is Grant's posterior estimate of Obie's true batting average?

\bigskip
\noindent
{\bf Part (e)} Both Joe and Grant had the same expected value for their prior distributions, and they both observed the same data? Why were their posterior estimates different?


\newpage
\subsection*{Problem Solution}
\bigskip
\noindent
{\bf Part (a)} What are the expected values for both Joe and Grant's prior distributions?
\bigskip
\noindent
{\bf Solution}\\
Joe:
\begin{align*}
E[p] &= \frac{a}{a + b}\\
&= \frac{20}{20 + 60} = 0.25\\
\end{align*}
Grant:
\begin{align*}
E[p] &= \frac{a}{a + b}\\
&= \frac{75}{75 + 225} = 0.25\\
\end{align*}


\noindent
{\bf Part (b)} What are the standard deviations for both Joe and Grant's prior distributions?

\bigskip
\noindent
{\bf Solution} 



\newpage
\subsubsection*{Problem 4, continued}
Joe:
\begin{align*}
\sigma^2 &= \frac{ab}{(a + b + 1)(a + b)^2}\\
&= \frac{20 \cdot 60}{(20 + 60 + 1)\cdot(20 + 60)^2}\\
&= 0.002314815\\
\sigma &= 0.04811252\\
\end{align*}
Grant:
\begin{align*}
\sigma^2 &= \frac{ab}{(a + b + 1)(a + b)^2}\\
&= \frac{75 \cdot 225}{(75 + 225 + 1)\cdot(75 + 225)^2}\\
&= 0.0006229236\\
\sigma &= 0.02495844\\
\end{align*}
\noindent
{\bf Part (c)} Obie gets 80 hits in his first 200 at-bats. What is Joe's posterior estimate of Obie's true batting average?

\bigskip
\noindent
{\bf Solution} 
\begin{align*}
a' &= a + k = 20 + 80 = 100\\
b' &= b + N - k = 60 + 200 - 80 = 180\\
E[p|data] &= \frac{a'}{a' + b'}\\
&= \frac{100}{100 + 180} = 0.3571429
\end{align*}


\noindent
{\bf Part (d)} Again, Obie gets 80 hits in his first 200 at-bats. What is Grant's posterior estimate of Obie's true batting average?

\bigskip
\noindent
{\bf Solution} 
\begin{align*}
a' &= a + k = 75 + 80 = 155\\
b' &= b + N - k = 225 + 200 - 80 = 345\\
E[p|data] &= \frac{a'}{a' + b'}\\
&= \frac{155}{155 + 345} = 0.31
\end{align*}


\noindent
{\bf Part (e)} Both Joe and Grant had the same expected value for their prior distributions, and they both observed the same data? Why were their posterior estimates different?

\bigskip
\noindent
{\bf Solution} 

Two distribution's having the same expected value is not a sufficient condition. They had different prior distributions (specifically Joe's distribution is more dispersed) but the same observed data, so their posteriors can be different.






\newpage
\section*{Problem 5}

\subsection*{Problem Statement}

Obie is consistently batting .400 i.e.\ 40\% of all his at-bats result in hits. Tyrone started the season with a prior for Obie's batting average that was a beta distribution with parameters $a = 27$ and $b = 73$.

\bigskip
\noindent
{\bf Problem} If Obie continues to consistently get hits for 40\% of his at-bats, what is the number of at-bats that he will need before Tyrone's estimate of Obie's true batting average is at least .350? That is, if $x$ is the number of at-bats, how large does $x$ have to be so that when Obie gets $0.4x$ hits, Tyrone's estimate of Obie's true probability of success is at least 35\%?


\subsection*{Problem Solution}
\begin{align*}
E[p|data] &= 0.35 = \frac{a'}{a' + b'}\\
k &= 0.4N\\
a' &= a + k = a + 0.4N = 27 + 0.4N\\
b' &= b + N - k = b + N - 0.4N = b + 0.6N = 73 + 0.6N\\
0.35 &= \frac{27 + 0.4N}{27 + 0.4N + 73 + 0.6N}\\
N &= 160
\end{align*}



\newpage
\section*{Problem 6}

\subsection*{Problem Statement}

Suppose the outcome random variable $X$ has a Poisson distribution with parameter $\mu$:
$$
\Pr(X = k\ |\ \mu) = \frac{ e^{-\mu} \cdot \mu^k}{k!}
$$
Also, suppose that $\mu$ has an exponential distribution with parameter $\lambda$:
$$
f(\mu) = \lambda \cdot e^{-\lambda \mu}
$$
Using the Law of Total Probability, show that the unconditional distribution of $X$ is:
$$
\Pr( X = k ) = \frac{ \lambda }{(\lambda + 1)^{k+1}}
$$
{\bf Hint} This problem is very similar to the example in Section 1, Case 2, in the reading, starting at page on page 4. If you're unsure about how to procede, reviewing this section would be a great way to get started.


\subsection*{Problem Solution}
\begin{align*}
P(X=k) &= \int_0^\infty \frac{e^{-\mu} \cdot \mu^k}{k!} \lambda e^{-\lambda \mu} d\mu\\
&= \frac{\lambda}{k!} \int_0^\infty \mu^k \cdot e^{-\lambda \mu - \mu}d\mu\\
s &= (\lambda + 1) \mu\\
\mu &= \frac{s}{\lambda + 1}\\
d\mu &= \frac{ds}{\lambda + 1}
\end{align*}


\newpage
\subsubsection*{Problem 6, continued}
\begin{align*}
P(X=k) &= \frac{\lambda}{k!} \int_0^\infty \biggr(\frac{s}{\lambda + 1}\biggr)^k \cdot e^{-s}\frac{ds}{\lambda + 1}\\
&= \frac{\lambda}{(\lambda + 1)^{k + 1} k!} \int_0^\infty s^k e^{-s}ds\\
&= \frac{\lambda}{(\lambda + 1)^{k + 1} k!} \Gamma(k + 1)\\
&= \frac{\lambda}{(\lambda + 1)^{k + 1}}
\end{align*}


\newpage
\section*{Problem 7}

\subsection*{Problem Statement}

Suppose the outcome random variable $X$ has an exponential distribution with parameter $\lambda$:
$$
f(x\ |\ \lambda) = \lambda  e^{-\lambda x}
$$
Next, suppose $\lambda$ also follows an exponential distribution, but with parameter $\theta$:
$$
g(\lambda\ |\ \theta) = \theta e^{-\theta \lambda}
$$
Using the Law of Total Probability, show that the unconditional density of $X$ is:
$$
f(x) = \frac{ \theta }{(x + \theta)^2}
$$
{\bf Hint} This problem is very similar to the example in Section 1, Case 3, in the reading, starting at page on page 6. If you're unsure about how to procede, reviewing this section would be a great way to get started.

\subsection*{Problem Solution}
\begin{align*}
f(x) &= \int_0^\infty \lambda e^{-\lambda x} \theta e^{-\theta\lambda}d\lambda\\
&= \theta \int_0^\infty \lambda e^{-\lambda x} e^{-\theta\lambda}d\lambda\\
&= \theta \int_0^\infty \lambda e^{-(x + \theta)\lambda}d\lambda\\
s &= (x + \theta)\lambda\\
\lambda &= \frac{s}{x + \theta}\\
d\lambda &= \frac{ds}{x + \theta}\\
\end{align*}

\newpage
\subsubsection*{Problem 7, continued}
\begin{align*}
f(x) &= \theta \int_0^\infty \frac{s}{x + \theta} e^{-s} \frac{ds}{x + \theta}\\
&= \frac{\theta}{(x + \theta)^2} \int_0^\infty se^{-s}ds\\
&= \frac{\theta}{(x + \theta)^2} \Gamma(2)\\
&= \frac{\theta}{(x + \theta)^2}
\end{align*}


\newpage
\section*{Problem 8}

\subsection*{Problem Statement}

Suppose the outcome random variable $X$ has a Poisson distribution with parameter $\mu$. This parameter has a prior distribution which is exponential, and this exponential distribution has parameter $\lambda$.

\bigskip
\noindent
{\bf Part (a)} Write down the explicit algebraic form of the density function for the prior distribution for the parameter $\mu$.

\bigskip
\noindent
{\bf Part (b)} Suppose we observe a value of $k$ for the outcome random variable $X$. Write down the explicit algebraic form of the likelihood.

\bigskip
\noindent
{\bf Part (c)} Show that the joint distribution is
$$
\hbox{Joint} \propto \mu^k \cdot e^{-(\lambda + 1) \mu}
$$

\bigskip
\noindent
{\bf Part (d)} Show that
$$
\int_0^\infty \mu^{k} \cdot e^{-(\lambda+1) \mu} \cdot d\mu = \frac{ \Gamma( k + 1 ) }{(\lambda + 1)^{k+1} }
$$
\noindent
{\bf Hint} We've seen this sort of integral a few times before. Try to make a substitution to turn this into a gamma function.

\bigskip
\noindent
{\bf Part (e)} What is the density function for the posterior distribution of $\mu$, given that the outcome variable $X$ had the observed value $k$?


\newpage
\subsection*{Problem Solution}

\noindent
{\bf Part (a)} Write down the explicit algebraic form of the density function for the prior distribution for the parameter $\mu$.

\bigskip
\noindent
{\bf Solution}
\begin{align*}
f(\mu) &= \lambda e^{-\lambda\mu}
\end{align*}


\noindent
{\bf Part (b)} Suppose we observe a value of $k$ for the outcome random variable $X$. Write down the explicit algebraic form of the likelihood.

\bigskip
\noindent
{\bf Solution}
\begin{align*}
g(X=k|\mu) &= \frac{e^{-\mu}\mu^k}{k!}
\end{align*}

\noindent
{\bf Part (c)} Show that the joint distribution is
$$
\hbox{Joint} \propto \mu^k \cdot e^{-(\lambda + 1) \mu}
$$


\bigskip
\noindent
{\bf Solution}
\begin{align*}
g(X=k|\mu)\cdot f(u) &= \frac{e^{-\mu}\mu^k}{k!} \lambda e^{-\lambda\mu}\\
&= \mu^k \cdot e^{-(\lambda + 1)\mu}\cdot\frac{\lambda}{k!}\\
&\propto \mu^k \cdot e^{-(\lambda + 1)\mu}
\end{align*}

\newpage

\subsubsection*{Problem 8, continued}

\noindent
{\bf Part (d)} Show that
$$
\int_0^\infty \mu^{k} \cdot e^{-(\lambda+1) \mu} \cdot d\mu = \frac{ \Gamma( k + 1 ) }{(\lambda + 1)^{k+1} }
$$
\noindent
{\bf Hint} We've seen this sort of integral a few times before. Try to make a substitution to turn this into a gamma function.

\bigskip
\noindent
{\bf Solution}
\begin{align*}
s &= (\lambda + 1)\mu\\
\mu &= \frac{s}{\lambda + 1}\\
d\mu &= \frac{ds}{\lambda + 1}\\
\int_0^\infty \mu^{k} \cdot e^{-(\lambda+1) \mu} \cdot d\mu &= \int_0^\infty \biggr(\frac{s}{\lambda + 1}\biggr)^{k} \cdot e^{-s} \cdot \frac{ds}{\lambda + 1}\\
&= \frac{1}{(\lambda + 1)^{k + 1}} \int_0^\infty s^k e^{-s}ds\\
&= \frac{\Gamma(k + 1)}{(\lambda + 1)^{k + 1}}
\end{align*}



\newpage
\subsubsection*{Problem 8, continued}

\noindent
{\bf Part (e)} What is the density function for the posterior distribution of $\mu$, given that the outcome variable $X$ had the observed value $k$?

\bigskip
\noindent
{\bf Solution}
\begin{align*}
h(X=k \text{ and }\lambda) &= \int_0^\infty \frac{\Gamma(k + 1)}{(\lambda + 1)^{k + 1}} d\lambda\\
&= \Gamma(k + 1) \int_0^\infty \frac{1}{(\lambda + 1)^{k + 1}}d\lambda\\
&= \frac{\Gamma(k + 1)}{k(\lambda + 1)^k}\\
\pi(\mu|X=k) &= \frac{\frac{\Gamma(k + 1)}{(\lambda + 1)^{k + 1}}}{h(X=k \text{ and }\lambda)}\\
&= \frac{k}{\lambda + 1}
\end{align*}





\end{document}
